{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest CoverType 2c): Scikit-Learn\n",
    "\n",
    "> *This notebook works well with the `Python 3 (Data Science)` kernel in [SageMaker Studio](https://aws.amazon.com/sagemaker/studio/)*\n",
    "\n",
    "In this notebook, we'll tackle our Forest Cover Type classification problem using [**Scikit Learn's RandomForestClassifier**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import json\n",
    "import os\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.sklearn import SKLearn as SKLearnEstimator\n",
    "import seaborn as sn\n",
    "from sklearn import metrics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "# Local Dependencies:\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r experiment_name\n",
    "%store -r preproc_trial_component_name\n",
    "%store -r project_id\n",
    "%store -r target_model\n",
    "\n",
    "smclient = boto3.client(\"sagemaker\")\n",
    "role = sagemaker.get_execution_role()\n",
    "smsess = sagemaker.session.Session()\n",
    "\n",
    "project = util.project.init(project_id, role)\n",
    "print(project)\n",
    "\n",
    "sandbox_bucket = boto3.resource(\"s3\").Bucket(project.sandbox.sandbox_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "For the purposes of our **Experiment**, the (best outcome of the) SKLearn approach is one trial to be compared against other qualitatively different approaches.\n",
    "\n",
    "If we were planning to iterate a lot on model parameters, we may also decide to define **another Experiment** to capture our tests leading up to the higher-level business outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randfor_trial = Trial.create(\n",
    "    trial_name=util.append_timestamp(\"randfor-sklearn\"), \n",
    "    experiment_name=experiment_name,\n",
    "    sagemaker_boto_client=smclient,\n",
    ")\n",
    "randfor_trial.add_trial_component(preproc_trial_component_name)\n",
    "\n",
    "preproc_trial_component = TrialComponent.load(preproc_trial_component_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of the Estimator should be quite familiar to experienced framework container users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"target\": \"Cover_Type\",\n",
    "    \"seed\": 1337,\n",
    "}\n",
    "\n",
    "estimator = SKLearnEstimator(\n",
    "    role=role,\n",
    "    entry_point=\"main.py\",\n",
    "    source_dir=\"src_sklearn\",\n",
    "    framework_version=\"0.23-1\",\n",
    "    py_version=\"py3\",\n",
    "\n",
    "    base_job_name=\"forestcover-randfor\",\n",
    "    output_path=f\"s3://{sandbox_bucket.name}/trainjobs\",\n",
    "    checkpoint_s3_uri=f\"s3://{sandbox_bucket.name}/trainjobs\",\n",
    "\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=[\n",
    "        { \"Name\": \"train:accuracy\", \"Regex\": r\"train:accuracy=(.*?);\", },\n",
    "        { \"Name\": \"validation:accuracy\", \"Regex\": r\"validation:accuracy=(.*?);\", },\n",
    "    ],\n",
    "    enable_sagemaker_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...But one important difference is the `experiment_config` parameter for `.fit()` - which will automatically create special training job-linked Trial Components for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    inputs={\n",
    "        \"train\": preproc_trial_component.output_artifacts[\"train-csv\"].value,\n",
    "        \"validation\": preproc_trial_component.output_artifacts[\"validation-csv\"].value,\n",
    "    },\n",
    "    experiment_config={\n",
    "        # This will create a TrainingJob-linked TrialComponent and automatically attach hyperparameters etc\n",
    "        \"TrialName\": randfor_trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": \"Training\",\n",
    "    },\n",
    "    #wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR re-attach to previous training job by name, e.g:\n",
    "#estimator = estimator.attach(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model (Batch Transform)\n",
    "\n",
    "We've trained something, and we have validation metrics which should give us an idea of its performance: But at some point we probably want to test out inference to validate that everything's working OK (and maybe see what performance we can expect on our test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_desc = estimator.latest_training_job.describe()\n",
    "model_path = training_job_desc[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "model_name = training_job_desc[\"TrainingJobName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a SKLearn Transformer from the trained SKLearn Estimator\n",
    "transformer = estimator.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    strategy=\"MultiRecord\",\n",
    "    max_payload=1,  # 1MB\n",
    "    accept=\"text/csv\",  # Need to specify input and output types when using filters for .transform()\n",
    "    assemble_with=\"Line\",\n",
    "    output_path=f\"s3://{sandbox_bucket.name}/test/{model_name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API model is created now, so let's tag it straight away with Boto3:\n",
    "model_desc = smclient.describe_model(ModelName=transformer.model_name)\n",
    "smclient.add_tags(\n",
    "    ResourceArn=model_desc[\"ModelArn\"],\n",
    "    Tags=[\n",
    "        { \"Key\": \"ExperimentName\", \"Value\": experiment_name },\n",
    "        { \"Key\": \"TrialName\", \"Value\": randfor_trial.trial_name },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use SageMaker Batch Transform's [data processing functionality](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html) to control how data gets batched in to our algorithm, and [associate output predictions with the input data](https://aws.amazon.com/blogs/machine-learning/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform/) - which will make measuring accuracy much easier later.\n",
    "\n",
    "As with the training job, providing an `experiment_config` will enable the transform job to create us a **Trial Component** recording the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer.transform(\n",
    "    preproc_trial_component.output_artifacts[\"test-csv\"].value,\n",
    "    split_type=\"Line\",\n",
    "    content_type=\"text/csv\",  # Need to specify input and output types when using filters\n",
    "    # TODO: Check why -2 is required vs -1 per JSONPath spec for trimming last column\n",
    "    input_filter=\"$[:-2]\",  # Exclude target column from input to the model\n",
    "    join_source=\"Input\",  # Store both input and output in the result (saves us re-joining in notebook)\n",
    "    # No output_filter so our output will be all source columns (incl target) + all prediction columns\n",
    "    experiment_config={\n",
    "        \"ExperimentName\": experiment_name,\n",
    "        \"TrialName\": randfor_trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": \"Test\",\n",
    "    },\n",
    "    wait=True,\n",
    "    logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the transform complete, we simply need to download and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_root_filename = preproc_trial_component.output_artifacts[\"test-csv\"].value.rpartition(\"/\")[2]\n",
    "\n",
    "!mkdir -p data/test/$model_name\n",
    "sandbox_bucket.download_file(\n",
    "    f\"test/{model_name}/{test_root_filename}.out\",  # Batch Transform appends \".out\"\n",
    "    f\"data/test/{model_name}/{test_root_filename}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/columns.json\", \"r\") as f:\n",
    "    train_columns = json.load(f)\n",
    "\n",
    "# TODO: Save from data prep, as we did with columns\n",
    "# Note our first cover_type is a dummy because the dataset's encoding starts at 1.\n",
    "cover_types = (\"N/A\", \"Spruce/Fir\", \"Lodgepole Pine\", \"Ponderosa Pine\", \"Cottonwood/Willow\", \"Aspen\", \"Douglas-fir\", \"Krummholz\")\n",
    "\n",
    "#result_cols = train_columns[:-1] + [\"Actual_Cover_Type\"] + [\"Pred_Cover_Type\"]\n",
    "result_cols = train_columns[:-1] + [\"Actual_Cover_Type\"] + [\"Pred \" + typ for typ in cover_types[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_results = pd.read_csv(\n",
    "    f\"data/test/{model_name}/{test_root_filename}\",\n",
    "    names=result_cols\n",
    ")\n",
    "\n",
    "print(f\"Shape: {df_test_results.shape}\")\n",
    "df_test_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results just have class probabilities: Recover the predicted class column (argmax):\n",
    "df_pred_probs = df_test_results[[\"Pred \" + typ for typ in cover_types[1:]]]\n",
    "# idxmax axis=1 returns index values (i.e. column names) - so rename columns to numbers:\n",
    "predicted_classes = df_pred_probs.rename(\n",
    "    columns={x:y for x,y in zip(df_pred_probs.columns,range(1,1+len(df_pred_probs.columns)))}\n",
    ").idxmax(axis=1)\n",
    "\n",
    "# Now we can add the post-processed results into the main dataframe:\n",
    "df_test_results[\"Pred_Cover_Type\"] = predicted_classes\n",
    "df_test_results[\"Pred_Correct\"] = df_test_results[\"Pred_Cover_Type\"] == df_test_results[\"Actual_Cover_Type\"]\n",
    "df_test_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix and accuracy summary:\n",
    "confusion = metrics.confusion_matrix(df_test_results[\"Actual_Cover_Type\"], df_test_results[\"Pred_Cover_Type\"])\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(\n",
    "    pd.DataFrame(\n",
    "        confusion,\n",
    "        index = cover_types[1:],\n",
    "        columns = cover_types[1:],\n",
    "    ),\n",
    "    annot=True\n",
    ")\n",
    "\n",
    "n_correct = sum(df_test_results[\"Pred_Correct\"])\n",
    "n_tested = len(df_test_results)\n",
    "print(f\"{n_correct} of {n_tested} samples correct: Accuracy={n_correct/n_tested:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-1:102112518831:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
