{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Cover Type with SageMaker Experiments - Introduction\n",
    "\n",
    "This series of notebooks demonstrates techniques for tabular data ML in SageMaker, on the popular **\"Forest Cover Type\"** multiclass classification task.\n",
    "\n",
    "**This notebook** handles initial loading of the data and some basic transformations: After which you'll be ready to run the follow-on notebooks to train and deploy predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the task & Acknowledgements\n",
    "\n",
    "The Forest Cover Type dataset is copyright Jock A. Blackard and Colorado State University, and made available to us via the [**UCI Machine Learning Repository page**](https://archive.ics.uci.edu/ml/datasets/covertype).\n",
    "\n",
    "The task is to predict, for each of 581012 patches of forest in northern Colorado, which one of 7 types types of tree cover dominate from 54 numerical input variables.\n",
    "\n",
    "See [**Forest Cover Type Classification Study**](https://rstudio-pubs-static.s3.amazonaws.com/160297_f7bcb8d140b74bd19b758eb328344908.html) (Thomas Kolasa and Aravind Kolumum Raja) for a really nicely-presented review of the problem with traditional data science methods and interactive graphics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our environment\n",
    "\n",
    "Note that [sagemaker-experiments](https://github.com/aws/sagemaker-experiments) ([docs](https://sagemaker-experiments.readthedocs.io/en/latest/)) is a **separate library** from the [SageMaker SDK](https://sagemaker.readthedocs.io/en/stable/).\n",
    "\n",
    "We'll start with some installs to make these notebooks more consistent across different kernel environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker-experiments SDK is not installed on SageMaker notebooks by default.\n",
    "# tqdm is available in some kernels but not others, between Studio and Notebook Instances\n",
    "!pip install sagemaker-experiments tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the typical external libraries, we've provided a **set of prototype tools in the [/util](util) folder** sketching out some ideas for common cross-project utility code that can help simplify the workflow for regular use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import json\n",
    "import os\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "# Local Dependencies:\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll set some configurations, bearing in mind we are working in a data science sandbox within a broader (perhaps multi-user) project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "smclient = boto3.client(\"sagemaker\")\n",
    "smsess = sagemaker.session.Session()\n",
    "\n",
    "\n",
    "bucket_name = # TODO: Personal sandbox bucket\n",
    "%store bucket_name\n",
    "\n",
    "project_id = # TODO: Project ID from CloudFormation stack\n",
    "%store project_id\n",
    "\n",
    "\n",
    "bucket = boto3.resource(\"s3\").Bucket(bucket_name)\n",
    "\n",
    "project_config = util.project.init(project_id)  # Read project stack parameters from the AWS SSM store\n",
    "project_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create (or load) the **Experiment** in which we'll track our work:\n",
    "\n",
    "An **\"Experiment\"** is a **collection of comparable \"Trials\"**.\n",
    "\n",
    "Trials are more abstract and flexible than just training jobs, so the scope at which we define an \"Experiment\" will be determined by what we're trying to track and the capabilities of the Experiments functionality.\n",
    "\n",
    "In this context we'll be comparing different ways to solve our classification problem, and have taken the decision (arguably driven by this being a code sample not an actual project) that this experiment be repeatable over time - so have appended a timestamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = util.append_timestamp(\"forest-cover-type\")\n",
    "experiment = util.smexps.create_or_load_experiment(\n",
    "    experiment_name=experiment_name,\n",
    "    description=\"Classification of forest type from cartographic variables\",\n",
    "    sagemaker_boto_client=smclient,  # (Optional)\n",
    ")\n",
    "%store experiment_name\n",
    "print(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Trial Components\"** are **discrete sub-stages** (e.g. processing, or training jobs) with associated of a **\"Trial\"**.\n",
    "\n",
    "Trial Components can be associated to multiple trials, and that's exactly what we'll do here to record our initial **data pre-processing**.\n",
    "\n",
    "A **Tracker** is a utility to record data to a Trial Component, and can be used either:\n",
    "\n",
    "- As a Python context manager (with the `with` statement) for more stable-codified workflows (such as in the [public SM Experiments sample](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-experiments/mnist-handwritten-digits-classification-experiment.ipynb))\n",
    "- ...Or just as an interactive interface as we will here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_tracker = Tracker.create(\n",
    "    display_name=\"Preprocessing\",\n",
    "    sagemaker_boto_client=smclient,  # (Optional)\n",
    ")\n",
    "# (can also Tracker.load() from an existing Trial Component)\n",
    "\n",
    "preproc_trial_component_name = preproc_tracker.trial_component.trial_component_name\n",
    "%store preproc_trial_component_name\n",
    "\n",
    "print(preproc_tracker.trial_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of illustration we could create a \"Trial\" including **only** the pre-processing step... But that would be abusing our intention to use the Experiment to record comparable tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproc_trial = Trial.create(\n",
    "#     trial_name=util.append_timestamp(\"preproc-only\"), \n",
    "#     experiment_name=experiment.experiment_name,\n",
    "#     sagemaker_boto_client=smclient,\n",
    "# )\n",
    "# preproc_trial.add_trial_component(preproc_trial_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Explore the Data\n",
    "\n",
    "It's important to notice **we're sort of working backwards here**: The project environment has been provisioned, but the source data and target/test cases aren't loaded into it yet!\n",
    "\n",
    "How stable are your raw source data and target variable definitions over the lifetime of an ML project? In practice we can expect some balance between:\n",
    "- Working from target dataset+outcomes to a model\n",
    "- Refining input features and problem definition from experimental findings\n",
    "\n",
    "Here we'll show a pre-processing workflow with data logging to SageMaker Experiments, and then upload the final datasets to the project's common \"source\" bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_uri = \"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\n",
    "\n",
    "!mkdir -p data/raw\n",
    "!wget -O data/raw/covtype.data.gz $raw_data_uri\n",
    "!wget -O data/raw/covtype.info https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info\n",
    "!gunzip -f data/raw/covtype.data.gz\n",
    "assert os.path.isfile(\"data/raw/covtype.data\")  # (Some of the shell cmds can fail without raising error)\n",
    "\n",
    "preproc_tracker.log_input(\n",
    "    name=\"UCI-Covertype\",\n",
    "    media_type=\"text/csv\",\n",
    "    value=raw_data_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data format is documented in the [data/raw/covtype.info](data/raw/covtype.info) we just downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_types = (\"N/A\", \"Spruce/Fir\", \"Lodgepole Pine\", \"Ponderosa Pine\", \"Cottonwood/Willow\", \"Aspen\", \"Douglas-fir\", \"Krummholz\")\n",
    "wilderness_areas = (\"Rawah\", \"Neota\", \"Comanche Peak\", \"Cache la Poudre\")\n",
    "preproc_tracker.log_parameters({\n",
    "    \"cover_types\": cover_types,\n",
    "    \"n_cover_types\": len(cover_types) - 1,\n",
    "    \"wilderness_areas\": wilderness_areas,\n",
    "    \"n_wilderness_areas\": len(wilderness_areas),\n",
    "})\n",
    "\n",
    "df_raw = pd.read_csv(\n",
    "    \"data/raw/covtype.data\",\n",
    "    names=[\n",
    "        \"Elevation_m\",  # Elevation in meters\n",
    "        \"Aspect_deg\",  # Aspect in degrees azimuth\n",
    "        \"Slope_deg\",  # Slope in degrees\n",
    "        \"Horizontal_Distance_To_Hydrology_m\",  # Horz Dist to nearest surface water features\n",
    "        \"Vertical_Distance_To_Hydrology_m\",  # Vert Dist to nearest surface water features\n",
    "        \"Horizontal_Distance_To_Roadways_m\",  # Horz Dist to nearest roadway\n",
    "        \"Hillshade_9am_uint8\",  # Hillshade index at 9am, summer solstice\n",
    "        \"Hillshade_Noon_uint8\",  # Hillshade index at noon, summer soltice\n",
    "        \"Hillshade_3pm_uint8\",  # Hillshade index at 3pm, summer solstice\n",
    "        \"Horizontal_Distance_To_Fire_Points_m\",  # Horz Dist to nearest wildfire ignition points\n",
    "    ]\n",
    "    + [\"Area_is_{}\".format(area.replace(\" \", \"_\")) for area in wilderness_areas]\n",
    "    + [\"Soil_Type_is_{:02}\".format(typ) for typ in range(1, 41)]\n",
    "    + [\n",
    "        \"Cover_Type\",  # Forest Cover Type designation\n",
    "    ]\n",
    ")\n",
    "\n",
    "bool_columns = [col for col in df_raw.columns if \"_is_\" in col]\n",
    "\n",
    "print(f\"Raw dataframe shape (rows, cols) = {df_raw.shape}\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process and Split\n",
    "\n",
    "This data is already quite well-conditioned, so all we really need to do is split training, validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell from the PyTorch-Tabnet Forest CoverType demo notebook is not actually necessary because the data is\n",
    "# already clean:\n",
    "# https://github.com/dreamquark-ai/tabnet/blob/develop/forest_example.ipynb\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# categorical_columns = []\n",
    "# categorical_dims =  {}\n",
    "# for col in df_raw.columns[df_raw.dtypes == object]:\n",
    "#     print(col, df_raw[col].nunique())\n",
    "#     l_enc = LabelEncoder()\n",
    "#     # df_raw[col] = df_raw[col].fillna(\"Unknown\")\n",
    "#     df_raw[col] = l_enc.fit_transform(df_raw[col].values)\n",
    "#     categorical_columns.append(col)\n",
    "#     categorical_dims[col] = len(l_enc.classes_)\n",
    "\n",
    "# preproc_tracker.log_parameters({\n",
    "#     \"categorical_columns\": categorical_columns,\n",
    "#     \"normalization_std\": categorical_dims,\n",
    "# })\n",
    "\n",
    "df_all = df_raw  # No pre-processing to do, so we'll just map same ref to a different variable name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pct = 0.8\n",
    "val_pct = 0.1\n",
    "test_pct = 1. - train_pct - val_pct\n",
    "preproc_tracker.log_parameters({\n",
    "    \"train_pct\": train_pct,\n",
    "    \"val_pct\": val_pct,\n",
    "    \"test_pct\": test_pct,\n",
    "})\n",
    "\n",
    "df_train, df_val, df_test = np.split(\n",
    "    df_all.sample(frac=1),\n",
    "    [int(train_pct*len(df_all)), int((train_pct + val_pct)*len(df_all))]\n",
    ")\n",
    "\n",
    "print(f\"Split randomly into train={len(df_train):,}, validation={len(df_val):,}, test={len(df_test):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract a Deliberately Biased Testing Subset\n",
    "\n",
    "We'll extract a deliberately biased subset of our testing data to use later in demonstrating how SageMaker's **Model Monitoring** functionality can be applied to detect [Concept Drift](https://en.wikipedia.org/wiki/Concept_drift) over time after a live model endpoint is deployed.\n",
    "\n",
    "Specifically we'll use the top X% of test records by `Elevation_m`: which is known to strongly affect the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biascheck_field = \"Elevation_m\"\n",
    "biascheck_test_pct = .2\n",
    "preproc_tracker.log_parameters({\n",
    "    \"biascheck_field\": biascheck_field,\n",
    "    \"biascheck_test_pct\": biascheck_test_pct,\n",
    "})\n",
    "\n",
    "df_test_bias, _ = np.split(\n",
    "    df_test.sort_values(biascheck_field, ascending=False),\n",
    "    [int(biascheck_test_pct*len(df_test))]\n",
    ")\n",
    "# ...And re-randomize:\n",
    "df_test_bias = df_test_bias.sample(frac=1)\n",
    "\n",
    "# Simpler version of summary showing elevation only:\n",
    "# pd.DataFrame({\n",
    "#     \"Test Set Elevation_m\": df_test[\"Elevation_m\"].describe(),\n",
    "#     \"Biased Subset Elevation_m\": df_test_hielev[\"Elevation_m\"].describe(),\n",
    "# })\n",
    "\n",
    "# Create summaries:\n",
    "test_summary = df_test.describe()\n",
    "test_bias_summary = df_test_bias.describe()\n",
    "\n",
    "# Log summary metrics to Experiment:\n",
    "for (dsname, summary) in ((\"test\", test_summary), (\"biastest\", test_bias_summary)):\n",
    "    for (fname, field) in ((\"feature\", biascheck_field), (\"target\", \"Cover_Type\")):\n",
    "        for stat in (\"mean\", \"std\"):\n",
    "            preproc_tracker.log_metric(\n",
    "                f\"biascheck-{fname}-{dsname}-{stat}\",\n",
    "                summary[field][stat]\n",
    "            )\n",
    "\n",
    "# Present nested-column summary tables in the notebook:\n",
    "test_summ_cpy = test_summary.copy()\n",
    "test_summ_cpy.columns = pd.MultiIndex.from_product([[\"Test Set\"], test_summ_cpy.columns])\n",
    "test_bias_summ_cpy = test_bias_summary.copy()\n",
    "test_bias_summ_cpy.columns = pd.MultiIndex.from_product([[\"Biased Subset\"], test_bias_summ_cpy.columns])\n",
    "pd.concat([test_summ_cpy, test_bias_summ_cpy], axis=1).loc[:, pd.IndexSlice[:, [biascheck_field, \"Cover_Type\"]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Prepared Datasets to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's helpful to have our training and validation datasets with headers so we can reference columns by name\n",
    "# in training hyperparams:\n",
    "df_train.to_csv(\"data/train-withheader.csv\", index=False)\n",
    "df_val.to_csv(\"data/validation-withheader.csv\", index=False)\n",
    "\n",
    "# ...But useful to skip headers in our test datasets so we can push the files through batch transformations:\n",
    "df_test.to_csv(\"data/test-noheader.csv\", index=False, header=False)\n",
    "df_test_bias.to_csv(\"data/test-bias-noheader.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save our columns list as well, since some files don't include them:\n",
    "with open(\"data/columns.json\", \"w\") as f:\n",
    "    json.dump(df_train.columns.to_list(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The upload() function returns the created S3 URI, allowing for nice feed-in to logging:\n",
    "preproc_tracker.log_output(\n",
    "    \"columns\",\n",
    "    sagemaker.s3.S3Uploader.upload(\"data/columns.json\", f\"s3://{bucket_name}/data\"),\n",
    "    \"application/json\"\n",
    ")\n",
    "preproc_tracker.log_output(\n",
    "    \"train-csv\",\n",
    "    sagemaker.s3.S3Uploader.upload(\"data/train-withheader.csv\", f\"s3://{bucket_name}/data\"),\n",
    "    \"text/csv\"\n",
    ")\n",
    "preproc_tracker.log_output(\n",
    "    \"validation-csv\",\n",
    "    sagemaker.s3.S3Uploader.upload(\"data/validation-withheader.csv\", f\"s3://{bucket_name}/data\"),\n",
    "    \"text/csv\"\n",
    ")\n",
    "preproc_tracker.log_output(\n",
    "    \"test-csv\",\n",
    "    sagemaker.s3.S3Uploader.upload(\"data/test-noheader.csv\", f\"s3://{bucket_name}/data\"),\n",
    "    \"text/csv\"\n",
    ")\n",
    "preproc_tracker.log_output(\n",
    "    \"test-biased-csv\",\n",
    "    sagemaker.s3.S3Uploader.upload(\"data/test-bias-noheader.csv\", f\"s3://{bucket_name}/data\"),\n",
    "    \"text/csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double-check everything saves OK, because otherwise errors can be silent:\n",
    "preproc_tracker.trial_component.save()\n",
    "\n",
    "# Shouldn't need to do these as well, but could:\n",
    "#preproc_trial.save()\n",
    "#experiment.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promoting our problem definition from sandbox to project\n",
    "\n",
    "This division and format of training, validation, and test data will be our current benchmark for the project.\n",
    "\n",
    "We can use IAM roles to control access between individual sandbox and project-shared resources; and even provide separate-but-assumable roles for a \"sudo\"-style safety mechanism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudosess = util.boto.assumed_role_session(\n",
    "    project_config[\"SudoRole\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudosess.resource(\"s3\").Bucket(project_config[\"SourceBucket\"]).upload_file(\n",
    "    \"data/test-noheader.csv\",\n",
    "    \"data/test-noheader.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
