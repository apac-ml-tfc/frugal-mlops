{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Cover Type with SageMaker Experiments - Introduction\n",
    "\n",
    "This series of notebooks demonstrates techniques for tabular data ML in SageMaker, on the popular **\"Forest Cover Type\"** multiclass classification task.\n",
    "\n",
    "**This notebook** handles initial loading of the data and some basic transformations: After which you'll be ready to run the follow-on notebooks to train and deploy predictive models.\n",
    "\n",
    "## Contents\n",
    "\n",
    "TODO: Maybe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the task & Acknowledgements\n",
    "\n",
    "The Forest Cover Type dataset is copyright Jock A. Blackard and Colorado State University, and made available to us via the [**UCI Machine Learning Repository page**](https://archive.ics.uci.edu/ml/datasets/covertype).\n",
    "\n",
    "The task is to predict, for each of 581012 patches of forest in northern Colorado, which of 7 types types of tree cover dominate.\n",
    "\n",
    "See [**Forest Cover Type Classification Study**](https://rstudio-pubs-static.s3.amazonaws.com/160297_f7bcb8d140b74bd19b758eb328344908.html) (Thomas Kolasa and Aravind Kolumum Raja) for a really nicely-presented review of the problem with traditional data science methods and interactive graphics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Experiments SDK is not installed on SageMaker notebooks by default:\n",
    "!pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import os\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "# Local Dependencies:\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "smclient = boto3.client(\"sagemaker\")\n",
    "smsess = sagemaker.session.Session()\n",
    "\n",
    "\n",
    "bucket_name = # TODO: Bucket\n",
    "%store bucket_name\n",
    "\n",
    "bucket = boto3.resource(\"s3\").Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create (or load) the **Experiment** in which we'll track our work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = util.append_timestamp(\"forest-cover-type\")\n",
    "experiment = util.smexps.create_or_load_experiment(\n",
    "    experiment_name=experiment_name,\n",
    "    description=\"Classification of forest type from cartographic variables\",\n",
    "    sagemaker_boto_client=smclient,  # (Optional)\n",
    ")\n",
    "%store experiment_name\n",
    "print(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_tracker = Tracker.create(\n",
    "    display_name=\"Preprocessing\",\n",
    "    sagemaker_boto_client=smclient,  # (Optional)\n",
    ")\n",
    "\n",
    "preproc_trial_component_name = preproc_tracker.trial_component.trial_component_name\n",
    "%store preproc_trial_component_name\n",
    "\n",
    "print(preproc_tracker.trial_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of illustration we could create a \"Trial\" including **only** the pre-processing step... But that'd be weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproc_trial = Trial.create(\n",
    "#     trial_name=util.append_timestamp(\"preproc-only\"), \n",
    "#     experiment_name=experiment.experiment_name,\n",
    "#     sagemaker_boto_client=smclient,\n",
    "# )\n",
    "# preproc_trial.add_trial_component(preproc_trial_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Explore the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_uri = \"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\n",
    "\n",
    "!mkdir -p data\n",
    "!wget -O data/covtype.data.gz $raw_data_uri\n",
    "!wget -O data/covtype.info https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info\n",
    "!gunzip -f data/covtype.data.gz\n",
    "assert os.path.isfile(\"data/covtype.data\")  # (Because some of the shell cmds can fail without raising error)\n",
    "\n",
    "preproc_tracker.log_input(\n",
    "    name=\"UCI-Covertype\",\n",
    "    media_type=\"text/csv\",\n",
    "    value=raw_data_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data format is documented in the [data/covtype.info](data/covtype.info) we just downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_types = (\"N/A\", \"Spruce/Fir\", \"Lodgepole Pine\", \"Ponderosa Pine\", \"Cottonwood/Willow\", \"Aspen\", \"Douglas-fir\", \"Krummholz\")\n",
    "wilderness_areas = (\"Rawah\", \"Neota\", \"Comanche Peak\", \"Cache la Poudre\")\n",
    "preproc_tracker.log_parameters({\n",
    "    \"cover_types\": cover_types,\n",
    "    \"n_cover_types\": len(cover_types) - 1,\n",
    "    \"wilderness_areas\": wilderness_areas,\n",
    "    \"n_wilderness_areas\": len(wilderness_areas),\n",
    "})\n",
    "\n",
    "df_raw = pd.read_csv(\n",
    "    \"data/covtype.data\",\n",
    "    names=[\n",
    "        \"Elevation_m\",  # Elevation in meters\n",
    "        \"Aspect_deg\",  # Aspect in degrees azimuth\n",
    "        \"Slope_deg\",  # Slope in degrees\n",
    "        \"Horizontal_Distance_To_Hydrology_m\",  # Horz Dist to nearest surface water features\n",
    "        \"Vertical_Distance_To_Hydrology_m\",  # Vert Dist to nearest surface water features\n",
    "        \"Horizontal_Distance_To_Roadways_m\",  # Horz Dist to nearest roadway\n",
    "        \"Hillshade_9am_uint8\",  # Hillshade index at 9am, summer solstice\n",
    "        \"Hillshade_Noon_uint8\",  # Hillshade index at noon, summer soltice\n",
    "        \"Hillshade_3pm_uint8\",  # Hillshade index at 3pm, summer solstice\n",
    "        \"Horizontal_Distance_To_Fire_Points_m\",  # Horz Dist to nearest wildfire ignition points\n",
    "    ]\n",
    "    + [\"Area_is_{}\".format(area.replace(\" \", \"_\")) for area in wilderness_areas]\n",
    "    + [\"Soil_Type_is_{:02}\".format(typ) for typ in range(1, 41)]\n",
    "    + [\n",
    "        \"Cover_Type\",  # Forest Cover Type designation\n",
    "    ]\n",
    ")\n",
    "\n",
    "bool_columns = [col for col in df_raw.columns if \"_is_\" in col]\n",
    "\n",
    "\n",
    "# we can log the s3 uri to the dataset we just uploaded\n",
    "\n",
    "print(f\"Raw dataframe shape (rows, cols) = {df_raw.shape}\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell from the DreamQuark Forest CoverType demo notebook is not actually necessary because the data is\n",
    "# already clean:\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# categorical_columns = []\n",
    "# categorical_dims =  {}\n",
    "# for col in df_raw.columns[df_raw.dtypes == object]:\n",
    "#     print(col, df_raw[col].nunique())\n",
    "#     l_enc = LabelEncoder()\n",
    "#     # df_raw[col] = df_raw[col].fillna(\"Unknown\")\n",
    "#     df_raw[col] = l_enc.fit_transform(df_raw[col].values)\n",
    "#     categorical_columns.append(col)\n",
    "#     categorical_dims[col] = len(l_enc.classes_)\n",
    "\n",
    "# preproc_tracker.log_parameters({\n",
    "#     \"categorical_columns\": categorical_columns,\n",
    "#     \"normalization_std\": categorical_dims,\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pct = 0.8\n",
    "val_pct = 0.1\n",
    "test_pct = 1. - train_pct - val_pct\n",
    "preproc_tracker.log_parameters({\n",
    "    \"train_pct\": train_pct,\n",
    "    \"val_pct\": val_pct,\n",
    "    \"test_pct\": test_pct,\n",
    "})\n",
    "\n",
    "df_raw_train, df_raw_val, df_raw_test = np.split(\n",
    "    df_raw.sample(frac=1),\n",
    "    [int(train_pct*len(df_raw)), int((train_pct + val_pct)*len(df_raw))]\n",
    ")\n",
    "\n",
    "print(f\"Split randomly into train={len(df_raw_train):,}, validation={len(df_raw_val):,}, test={len(df_raw_test):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract a Deliberately Biased Testing Subset\n",
    "\n",
    "We'll extract a deliberately biased subset of our testing data to use later in demonstrating how SageMaker's **Model Monitoring** functionality can be applied to detect [Concept Drift](https://en.wikipedia.org/wiki/Concept_drift) over time after a live model endpoint is deployed.\n",
    "\n",
    "Specifically we'll use the top 20% of test records by `Elevation_m`, the target variable is known to be strongly affected by this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biascheck_field = \"Elevation_m\"\n",
    "biascheck_test_pct = .2\n",
    "preproc_tracker.log_parameters({\n",
    "    \"biascheck_field\": biascheck_field,\n",
    "    \"biascheck_test_pct\": biascheck_test_pct,\n",
    "})\n",
    "\n",
    "df_raw_test_bias, _ = np.split(\n",
    "    df_raw_test.sort_values(biascheck_field, ascending=False),\n",
    "    [int(biascheck_test_pct*len(df_raw_test))]\n",
    ")\n",
    "# ...And re-randomize:\n",
    "df_raw_test_bias = df_raw_test_bias.sample(frac=1)\n",
    "\n",
    "# Simpler versioon of summary showing elevation only:\n",
    "# pd.DataFrame({\n",
    "#     \"Test Set Elevation_m\": df_raw_test[\"Elevation_m\"].describe(),\n",
    "#     \"Biased Subset Elevation_m\": df_raw_test_hielev[\"Elevation_m\"].describe(),\n",
    "# })\n",
    "\n",
    "# Create summaries:\n",
    "test_summary = df_raw_test.describe()\n",
    "test_bias_summary = df_raw_test_bias.describe()\n",
    "\n",
    "# Log summary metrics to Experiment:\n",
    "for (dsname, summary) in ((\"test\", test_summary), (\"biastest\", test_bias_summary)):\n",
    "    for (fname, field) in ((\"feature\", biascheck_field), (\"target\", \"Cover_Type\")):\n",
    "        for stat in (\"mean\", \"std\"):\n",
    "            preproc_tracker.log_metric(\n",
    "                f\"biascheck-{fname}-{dsname}-{stat}\",\n",
    "                summary[field][stat]\n",
    "            )\n",
    "\n",
    "# Present nested-column summary tables in the notebook:\n",
    "test_summ_cpy = test_summary.copy()\n",
    "test_summ_cpy.columns = pd.MultiIndex.from_product([[\"Test Set\"], test_summ_cpy.columns])\n",
    "test_bias_summ_cpy = test_bias_summary.copy()\n",
    "test_bias_summ_cpy.columns = pd.MultiIndex.from_product([[\"Biased Subset\"], test_bias_summ_cpy.columns])\n",
    "pd.concat([test_summ_cpy, test_bias_summ_cpy], axis=1).loc[:, pd.IndexSlice[:, [biascheck_field, \"Cover_Type\"]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Prepared Datasets to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_train.to_csv(\"data/train.csv\", index=False)\n",
    "df_raw_val.to_csv(\"data/validation.csv\", index=False)\n",
    "df_raw_test.to_csv(\"data/test.csv\", index=False)\n",
    "df_raw_test_bias.to_csv(\"data/test-bias.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The upload() function returns the created S3 URI, allowing for nice feed-in to logging:\n",
    "preproc_tracker.log_output(\n",
    "    \"train-csv\",\n",
    "    sagemaker.s3.S3Uploader.upload(\"data/train.csv\", f\"s3://{bucket_name}/data\"),\n",
    "    \"text/csv\"\n",
    ")\n",
    "preproc_tracker.log_output(\n",
    "    \"validation-csv\",\n",
    "    sagemaker.s3.S3Uploader.upload(\"data/validation.csv\", f\"s3://{bucket_name}/data\"),\n",
    "    \"text/csv\"\n",
    ")\n",
    "preproc_tracker.log_output(\n",
    "    \"test-csv\",\n",
    "    sagemaker.s3.S3Uploader.upload(\"data/test.csv\", f\"s3://{bucket_name}/data\"),\n",
    "    \"text/csv\"\n",
    ")\n",
    "preproc_tracker.log_output(\n",
    "    \"test-biased-csv\",\n",
    "    sagemaker.s3.S3Uploader.upload(\"data/test-bias.csv\", f\"s3://{bucket_name}/data\"),\n",
    "    \"text/csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't *think* these are necessary?\n",
    "#preproc_tracker.trial_component.save()\n",
    "#preproc_trial.save()\n",
    "#experiment.save()\n",
    "\n",
    "# TODO: Something is s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
