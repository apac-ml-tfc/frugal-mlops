{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Cover Type 2a): SageMaker Autopilot\n",
    "\n",
    "In this notebook, we'll tackle our Forest Cover Type classification problem using [**Amazon SageMaker Autopilot**](https://aws.amazon.com/sagemaker/autopilot/): A service that automatically trains and tunes the best machine learning models for classification or regression, based on your data while allowing to maintain full control and visibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and configuration\n",
    "\n",
    "This notebook recovers configuration saved with `%store` from the first notebook, so if you've restarted your notebook instance / image you may need to re-run cells from the first notebook to re-save the values. See the [storemagic docs](https://ipython.readthedocs.io/en/stable/config/extensions/storemagic.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import seaborn as sn\n",
    "from sklearn import metrics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "# Local Dependencies:\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r bucket_name\n",
    "%store -r experiment_name\n",
    "%store -r preproc_trial_component_name\n",
    "%store -r project_id\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket = boto3.resource(\"s3\").Bucket(bucket_name)\n",
    "role = sagemaker.get_execution_role()\n",
    "smclient = boto3.client(\"sagemaker\")\n",
    "smsess = sagemaker.session.Session()\n",
    "\n",
    "project_config = util.project.init(project_id)  # Read project stack parameters from the AWS SSM store\n",
    "print(project_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "For the purposes of **our Experiment**, the (best outcome of the) Autopilot approach is one trial to be compared against other qualitatively different approaches.\n",
    "\n",
    "Autopilot will automatically log **its own Experiment** describing the different candidate pre-processing and modelling configurations it explored: We can think of this as a lower-level experiment contributing towards our overall Forest Cover exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_trial = Trial.create(\n",
    "    trial_name=util.append_timestamp(\"autopilot\"), \n",
    "    experiment_name=experiment_name,\n",
    "    sagemaker_boto_client=smclient,\n",
    ")\n",
    "automl_trial.add_trial_component(preproc_trial_component_name)\n",
    "\n",
    "preproc_trial_component = TrialComponent.load(preproc_trial_component_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Or load existing trial instead)\n",
    "#automl_trial = Trial.load(\"autopilot-2020-07-28-05-41-14\")\n",
    "#preproc_trial_component = TrialComponent.load(preproc_trial_component_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the [high-level SageMaker SDK](https://sagemaker.readthedocs.io/en/stable/api/training/automl.html), defining and running an AutoML job is very similar to the `Estimator` API, but with higher-level parameters.\n",
    "\n",
    "As always, it's possible to use the lower-level, cross-AWS [boto3 SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to achieve the same results with usually more verbose code. The alternative boto3 syntax can be seen in the [official Autopilot samples](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/autopilot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoestimator = sagemaker.AutoML(\n",
    "    role=role,\n",
    "    sagemaker_session=smsess,\n",
    "    target_attribute_name=\"Cover_Type\",\n",
    "    problem_type=\"MulticlassClassification\",\n",
    "    job_objective={ \"MetricName\": \"Accuracy\" },\n",
    "    output_path=f\"s3://{bucket_name}/automl\",\n",
    "    base_job_name=\"auto-forestcover\",\n",
    "    max_candidates=30,\n",
    "    #max_runtime_per_training_job_in_seconds=None,\n",
    "    #total_job_runtime_in_seconds=None,\n",
    "    generate_candidate_definitions_only=False,\n",
    "    tags=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Owing to the amount of parallel experimentation going on, Autopilot log streams can be a bit much... Instead, we'll asynchronously kick off the job then produce a simple status spinner in the cell below.\n",
    "\n",
    "Note in particular that we **use the `preproc_trial_component` to set the source data location**: Anywhere we can directly create these links in our code will help to ensure the integrity of our records - even if cells are re-run in different orders during debugging and iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoestimator.fit(\n",
    "    [preproc_trial_component.output_artifacts[\"train-csv\"].value],\n",
    "    wait=False,\n",
    "    logs=False, #logs=True,  # Only works with wait=True\n",
    "    # Might want to set the job name explicitly because the default gives you very few free prefix chars!\n",
    "    #job_name=util.append_timestamp(\"auto-frstcv\"),\n",
    ")\n",
    "\n",
    "auto_ml_job_name = autoestimator.current_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Or attach to a previous AutoML job)\n",
    "#auto_ml_job_name = \"auto-for-2020-06-26-09-43-01-819\"\n",
    "#autoestimator = sagemaker.AutoML.attach(auto_ml_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_automl_status_done(status):\n",
    "    if status[\"AutoMLJobStatus\"] == \"Completed\":\n",
    "        return True\n",
    "    elif status[\"AutoMLJobStatus\"] in (\"Failed\", \"Stopped\"):\n",
    "        raise ValueError(f\"Job ended in non-successful state '{status['AutoMLJobStatus']}'\\n{status}\")\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "util.progress.polling_spinner(\n",
    "    autoestimator.describe_auto_ml_job,\n",
    "    is_automl_status_done,\n",
    "    fn_stringify_result=lambda status: f\"{status['AutoMLJobStatus']} - {status['AutoMLJobSecondaryStatus']}\",\n",
    "    spinner_secs=0.4,\n",
    "    poll_secs=30\n",
    ")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing the results\n",
    "\n",
    "The AutoML job generates a set of **candidate** solutions, each of which is typically a [pipeline model](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html) including feature pre- and post-processing containers as well as the core model container.\n",
    "\n",
    "There are lots of tools available in the SDK to explore the results, including:\n",
    "\n",
    "- Listing the candidate leaderboard\n",
    "- Downloading the auto-created notebooks\n",
    "- Drilling in to the detailed attributes and metrics for each candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description = autoestimator.describe_auto_ml_job()\n",
    "job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = autoestimator.list_candidates(\n",
    "    sort_by=\"FinalObjectiveMetricValue\",\n",
    "    sort_order=\"Descending\",\n",
    ")\n",
    "candidates_df = pd.DataFrame([\n",
    "    {\n",
    "        \"CandidateName\": candidate[\"CandidateName\"],\n",
    "        \"InferenceContainers\": len(candidate[\"InferenceContainers\"]),\n",
    "        \"MetricValue\": candidate[\"FinalAutoMLJobObjectiveMetric\"][\"Value\"],\n",
    "        \"MetricName\": candidate[\"FinalAutoMLJobObjectiveMetric\"][\"MetricName\"],\n",
    "        # (Plenty of other fields we could plot here if we wanted!)\n",
    "    }\n",
    "    for candidate in candidates\n",
    "])\n",
    "candidates_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the notebooks from the Autopilot job lets us actually see the data exploration and candidate model generation code - which we could use as a starting point to customize the candidates manually and improve performance even further.\n",
    "\n",
    "If there's time, you're encouraged to go open up these notebooks and see what's there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = os.path.join(\"data\", \"automl-results\", job_description[\"AutoMLJobName\"])\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "for item in job_description[\"AutoMLJobArtifacts\"]:\n",
    "    artifact_uri = job_description[\"AutoMLJobArtifacts\"][item]\n",
    "    artifact_name = item.replace(\"Location\", \"\")\n",
    "    artifact_bucket, artifact_key = util.boto.s3uri_to_bucket_and_key(artifact_uri)\n",
    "    filename = os.path.join(result_folder, artifact_key.rpartition(\"/\")[2])\n",
    "    print(f\"Downloading {artifact_name} to {filename}\")\n",
    "    s3.download_file(artifact_bucket, artifact_key, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing top model candidates (Batch Transform)\n",
    "\n",
    "Note that we could argue the comparison we're going to make here to TabNet isn't strictly fair (and the terminology might get a bit confusing), because Autopilot did its own train/validation splitting **within** the train dataset we gave it.\n",
    "\n",
    "However in the context of our experiment, we'll test the top **N** candidates proposed by Autopilot against our project's validation dataset - and compare their performance to the TabNet model in the next notebook.\n",
    "\n",
    "<div class=\"alert alert-info\"><b>Note:</b> We consider the <b>top N</b> candidates, rather than the winner alone, in case their performance on our validation dataset ranks differently than their observed metrics on Autopilot's \"validation\" split of the training dataset (which is all it had access to).</div>\n",
    "\n",
    "When creating models from Autopilot candidates, we can [configure the output types](https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development-container-output.html) as below - selecting e.g. just the predicted label, the prediction confidence, and/or the confidences associated with every label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TEST_CANDIDATES = 3  # Could be increased or reduced\n",
    "\n",
    "# We use predicted_label below for accuracy measurement, and also collect probabilities to be comparable to\n",
    "# the TabNet model in the next notebook:\n",
    "inference_response_keys = [\"predicted_label\", \"probabilities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "transformers = []\n",
    "\n",
    "# The candidates list is already sorted by metric (above)\n",
    "for candidate in candidates[0:N_TEST_CANDIDATES]:\n",
    "    model = autoestimator.create_model(\n",
    "        name=candidate[\"CandidateName\"],\n",
    "        candidate=candidate,\n",
    "        inference_response_keys=inference_response_keys,\n",
    "    )\n",
    "    models.append(model)\n",
    "\n",
    "    # Note the model isn't actually registered with the API until a transformer or endpoint is created:\n",
    "    # (because CPU vs GPU instance type affects the target container)\n",
    "    transformer = model.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        accept=\"text/csv\",  # Need to specify input and output types when using filters (below)\n",
    "        assemble_with=\"Line\",  # Join the predictions back together in to one output file with newlines\n",
    "        output_path=f\"s3://{bucket_name}/automl/test-transforms/{candidate['CandidateName']}/\",\n",
    "    )\n",
    "    transformers.append(transformer)\n",
    "\n",
    "    # Now the model has been registered, we can tag it:\n",
    "    model_desc = smclient.describe_model(ModelName=transformer.model_name)\n",
    "    smclient.add_tags(\n",
    "        ResourceArn=model_desc[\"ModelArn\"],\n",
    "        Tags=[\n",
    "            { \"Key\": \"ExperimentName\", \"Value\": experiment_name },\n",
    "            { \"Key\": \"TrialName\", \"Value\": automl_trial.trial_name },\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to kick off each candidate's transform job.\n",
    "\n",
    "Note that we:\n",
    "\n",
    "- Use our `TrialComponent` tracking as the record of the test CSV's location\n",
    "- Filter out the target column (we don't want to give the model the answer!)\n",
    "- **Join** the model outputs back on to the input columns in the result CSV - to save ourselves having to reconcile later here in the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, transformer in enumerate(transformers):\n",
    "    transformer.transform(\n",
    "        preproc_trial_component.output_artifacts[\"test-csv\"].value,\n",
    "        split_type=\"Line\",\n",
    "        content_type=\"text/csv\",  # Need to specify input and output types when using filters\n",
    "        # TODO: Check why -2 is required vs -1 per JSONPath spec for trimming last column\n",
    "        input_filter=\"$[:-2]\",  # Exclude target column from input to the model\n",
    "        join_source=\"Input\",  # Store both input and output in the result (saves us re-joining in notebook)\n",
    "        # No output_filter so our output will be all source columns (incl target) + all prediction columns\n",
    "        experiment_config={\n",
    "            \"ExperimentName\": experiment_name,\n",
    "            \"TrialName\": automl_trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": f\"Test-{candidates[ix]['CandidateName']}\",\n",
    "        },\n",
    "        wait=False,\n",
    "        logs=False,\n",
    "    )\n",
    "    print(f\"Started transform job {transformer._current_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...And then wait for them all to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for transformer in transformers:\n",
    "    transformer.wait(logs=False)\n",
    "print(\"All test transform jobs complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each candidate should now have a transform output CSV containing:\n",
    "\n",
    "- All the columns from the input dataset\n",
    "- The target column (which we filtered from going to the model with `input_filter`, but still gets passed through to the output file\n",
    "- The outputs we requested including `predicted_label` and `probabilities`\n",
    "\n",
    "Here we'll configure a loader to read in and transform the results spreadsheets, and then loop through the candidates printing out some metrics and visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of training columns was saved in data prep:\n",
    "with open(\"data/columns.json\", \"r\") as f:\n",
    "    train_columns = json.load(f)\n",
    "\n",
    "# TODO: Save from data prep, as we did with columns\n",
    "# Note our first cover_type is a dummy because the dataset's encoding starts at 1.\n",
    "cover_types = (\"N/A\", \"Spruce/Fir\", \"Lodgepole Pine\", \"Ponderosa Pine\", \"Cottonwood/Willow\", \"Aspen\", \"Douglas-fir\", \"Krummholz\")\n",
    "\n",
    "def standardize_results_df(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Function to load a test transform result from CSV and load/standardize the columns\"\"\"\n",
    "    df_test_results = pd.read_csv(\n",
    "        filepath,\n",
    "        names=train_columns[:-1] + [\"Actual_Cover_Type\"] + inference_response_keys,\n",
    "    )\n",
    "\n",
    "    if \"probabilities\" in inference_response_keys:\n",
    "        # By default this field is a JSON-stringified array of numbers, so we'll unpack it into a dataframe\n",
    "        # and name the columns. (See https://stackoverflow.com/a/36816769/13352657 for .apply(pd.Series))\n",
    "        probs_df = df_test_results[\"probabilities\"].apply(json.loads).apply(pd.Series)\n",
    "        probs_df.columns = [\"Pred \" + typ for typ in cover_types[1:]]\n",
    "        df_test_results.drop(columns=[\"probabilities\"], inplace=True)\n",
    "        df_test_results = pd.concat([df_test_results, probs_df], axis=1)\n",
    "\n",
    "    if \"predicted_label\" in inference_response_keys:\n",
    "        df_test_results.rename(columns={ \"predicted_label\": \"Pred_Cover_Type\" }, inplace=True)\n",
    "        df_test_results[\"Pred_Correct\"] = (\n",
    "            df_test_results[\"Pred_Cover_Type\"] == df_test_results[\"Actual_Cover_Type\"]\n",
    "        )\n",
    "        \n",
    "    return df_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "test_root_filename = preproc_trial_component.output_artifacts[\"test-csv\"].value.rpartition(\"/\")[2]\n",
    "for ix, transnformer in enumerate(transformers):\n",
    "    print(\"Analysing result for model: {}\".format(transformer.model_name))\n",
    "    os.makedirs(f\"data/test/{transformer.model_name}\", exist_ok=True)\n",
    "    candidate = candidates[ix]\n",
    "    bucket.download_file(\n",
    "        f\"automl/test-transforms/{candidate['CandidateName']}/{test_root_filename}.out\",  # Batch Transform appends \".out\"\n",
    "        f\"data/test/{transformer.model_name}/{test_root_filename}\",\n",
    "    )\n",
    "\n",
    "    df_test_results = standardize_results_df(f\"data/test/{transformer.model_name}/{test_root_filename}\")\n",
    "\n",
    "    n_correct = sum(df_test_results[\"Pred_Correct\"])\n",
    "    n_tested = len(df_test_results)\n",
    "    print(f\"{n_correct} of {n_tested} samples correct: Accuracy={n_correct/n_tested:.3%}\")\n",
    "    scores.append(n_correct/n_tested)\n",
    "\n",
    "    confusion = metrics.confusion_matrix(df_test_results[\"Actual_Cover_Type\"], df_test_results[\"Pred_Cover_Type\"])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(\n",
    "        pd.DataFrame(\n",
    "            confusion,\n",
    "            index = cover_types[1:],\n",
    "            columns = cover_types[1:],\n",
    "        ),\n",
    "        annot=True\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "ixbest = np.argmax(scores)\n",
    "print(f\"\\nBest model ix {ixbest}: {models[ixbest].name}\\nwith score of {scores[ixbest]:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have [confusion matrices](https://en.wikipedia.org/wiki/Confusion_matrix) and accuracy scores for each of the top-N candidate models that we brought forward for analysis.\n",
    "\n",
    "You might have found the index 0 was the \"best\" model by our testing set - which is encouraging, because it means Autopilot's metrics (from internal validation) generalized well to the unseen test dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Results in Our Experiment\n",
    "\n",
    "Because Autopilot is internally experimenting with different featurizations and algorithm configurations, it **creates its own Experiment** with associated Trials and Trial Components describing the detail of the flow it undertook.\n",
    "\n",
    "As we've seen so far, there's a lot of tracking data available for us to explore the Autopilot results and (further refine on the candidates it produced).\n",
    "\n",
    "For the purposes of **our Experiment** though (as created in Notebook 1) - which is to compare Autopilot with other methods - the Autopilot run is just one Trial and we only care about the best/selected outputs.\n",
    "\n",
    "We've already logged our N test transforms (`transform()` calls above), and the pre-processing step, and there are a couple of different strategies we could take for logging other aspects:\n",
    "\n",
    "- Verbose: Copy all Trial Components from the *winning* Autopilot Trial into our AutoML Trial\n",
    "- Concise: Create a custom Trial Component called something like \"Training\" which just logs the fact that models were created via Autopilot (including what parameters were provided), and links to the Autopilot Experiment\n",
    "\n",
    "Below we show the Verbose approach. For the Concise alternative, you'd be creating a custom Trial Component and adding parameters/artifacts, much like we did for Pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe_auto_ml_job() doesn't seem to give us anything to reconstruct what the Experiment name is, so\n",
    "# we'll assume it was created with the AutoML job name + standard suffix:\n",
    "automl_experiment = Experiment.load(f\"{auto_ml_job_name}-aws-auto-ml-job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This ignores our ixbest, in case it wasn't 0:\n",
    "best_candidate_name = job_description[\"BestCandidate\"][\"CandidateName\"]\n",
    "print(f\"Searching for {best_candidate_name} in logged Experiment...\")\n",
    "\n",
    "matching_trials = [\n",
    "    t for t in automl_experiment.list_trials() if t.display_name == f\"{best_candidate_name}-aws-trial\"\n",
    "]\n",
    "n_matching_trials = len(matching_trials)\n",
    "\n",
    "if n_matching_trials > 1:\n",
    "    raise ValueError(\"Found {} possible AutoML trials for best candidate {}:\\n\\n{}\".format(\n",
    "        n_matching_trials,\n",
    "        best_candidate_name,\n",
    "        matching_trials,\n",
    "    ))\n",
    "elif n_matching_trials < 1:\n",
    "    raise ValueError(\"Couldn't find AutoML trial matching candidate {}:\\n\\n{}\".format(\n",
    "        best_candidate_name,\n",
    "        list(automl_experiment.list_trials()),\n",
    "    ))\n",
    "\n",
    "matching_trial = Trial.load(matching_trials[0].trial_name)\n",
    "print(f\"Found exactly one matching trial:\\n{matching_trial.trial_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for component in matching_trial.list_trial_components():\n",
    "    automl_trial.add_trial_component(component.trial_component_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline-based deployment\n",
    "\n",
    "We'll skip over adding the Autopilot model to our project model registry and submitting it for production deployment here, and revisit the topic in the next notebook because the model artifact is a simpler, single-container model rather than an inference pipeline!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick demo of real-time deployment\n",
    "\n",
    "Our main workflow is to test the model against an offline dataset, and in general the SageMaker architecture makes batch transforms and real-time deployment pretty much interchangeable with no code change.\n",
    "\n",
    "However, there might be some cases where we want to quickly experiment with deploying our model to a test endpoint in case the real-time feed format is intended to be slightly different from the offline dataset.\n",
    "\n",
    "That direct deployment could look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    predictor.delete_endpoint()\n",
    "    time.sleep(5)  # (Otherwise can trigger errors if creating again immediately)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "predictor = autoestimator.deploy(\n",
    "    endpoint_name=auto_ml_job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    inference_response_keys=inference_response_keys,\n",
    "    predictor_cls=sagemaker.predictor.RealTimePredictor,\n",
    "    #wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Or attach to an existing endpoint)\n",
    "# predictor = sagemaker.predictor.RealTimePredictor(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we're using the default RealTimePredictor class, we need to explicitly configure for CSV:\n",
    "predictor.accept = \"text/csv\"\n",
    "predictor.content_type = \"text/csv\"\n",
    "predictor.serializer = sagemaker.predictor.csv_serializer\n",
    "predictor.deserializer = sagemaker.predictor.csv_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/columns.json\", \"r\") as f:\n",
    "    train_columns = json.load(f)\n",
    "df_test = pd.read_csv(\n",
    "    \"data/test-noheader.csv\",\n",
    "    names=train_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The csv_serializer is capable of processing array-like objects, so we'll use Pandas to filter our data\n",
    "# (remove target column and send in only a small batch of rows), but then convert to numpy:\n",
    "result = predictor.predict(df_test.drop(\"Cover_Type\", axis=1).iloc[0:10].to_numpy())\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `probabilities` are reported in the same JSON-stringified array format as we saw earlier when interpreting the batch transform results, so these would need to be unpacked for numerical analysis."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
