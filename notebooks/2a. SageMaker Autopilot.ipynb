{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Cover Type 2a): SageMaker Autopilot\n",
    "\n",
    "In this notebook, we'll tackle our Forest Cover Type classification problem using [**Amazon SageMaker Autopilot**](https://aws.amazon.com/sagemaker/autopilot/): A service that automatically trains and tunes the best machine learning models for classification or regression, based on your data while allowing to maintain full control and visibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "# Local Dependencies:\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r bucket_name\n",
    "%store -r experiment_name\n",
    "%store -r preproc_trial_component_name\n",
    "%store -r project_id\n",
    "\n",
    "bucket = boto3.resource(\"s3\").Bucket(bucket_name)\n",
    "role = sagemaker.get_execution_role()\n",
    "smclient = boto3.client(\"sagemaker\")\n",
    "smsess = sagemaker.session.Session()\n",
    "\n",
    "project_config = util.project.init(project_id)  # Read project stack parameters from the AWS SSM store\n",
    "print(project_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "For the purposes of **our Experiment**, the (best outcome of the) Autopilot approach is one trial to be compared against other qualitatively different approaches.\n",
    "\n",
    "Autopilot will automatically log **its own Experiment** describing the different candidate pre-processing and modelling configurations it explored: We can think of this as a lower-level experiment contributing towards our overall Forest Cover exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_trial = Trial.create(\n",
    "    trial_name=util.append_timestamp(\"autopilot\"), \n",
    "    experiment_name=experiment_name,\n",
    "    sagemaker_boto_client=smclient,\n",
    ")\n",
    "automl_trial.add_trial_component(preproc_trial_component_name)\n",
    "\n",
    "preproc_trial_component = TrialComponent.load(preproc_trial_component_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the [high-level SageMaker SDK](https://sagemaker.readthedocs.io/en/stable/api/training/automl.html), defining and running an AutoML job is very similar to the `Estimator` API, but with higher-level parameters.\n",
    "\n",
    "As always, it's possible to use the lower-level, cross-AWS [boto3 SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to achieve the same results with usually more verbose code. The alternative boto3 syntax can be seen in the [official Autopilot samples](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/autopilot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoestimator = sagemaker.automl.automl.AutoML(\n",
    "    role=role,\n",
    "    sagemaker_session=smsess,\n",
    "    target_attribute_name=\"Cover_Type\",\n",
    "    problem_type=\"MulticlassClassification\",\n",
    "    job_objective={ \"MetricName\": \"Accuracy\" },\n",
    "    output_path=f\"s3://{bucket_name}/automl\",\n",
    "    base_job_name=\"auto-forestcover\",\n",
    "    max_candidates=30,\n",
    "    #max_runtime_per_training_job_in_seconds=None,\n",
    "    #total_job_runtime_in_seconds=None,\n",
    "    generate_candidate_definitions_only=False,\n",
    "    tags=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Owing to the amount of parallel experimentation going on, Autopilot log streams can be a bit much... Instead, we'll asynchronously kick off the job then produce a simple status spinner in the cell below.\n",
    "\n",
    "Note in particular that we **use the `preproc_trial_component` to set the source data location**: Anywhere we can directly create these links in our code will help to ensure the integrity of our records - even if cells are re-run in different orders during debugging and iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoestimator.fit(\n",
    "    [preproc_trial_component.output_artifacts[\"train-csv\"].value],\n",
    "    wait=False,\n",
    "    logs=False, #logs=True,  # Only works with wait=True\n",
    "    # Might want to set the job name explicitly because the default gives you very few free prefix chars!\n",
    "    #job_name=util.append_timestamp(\"auto-frstcv\"),\n",
    ")\n",
    "\n",
    "auto_ml_job_name = autoestimator.current_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_automl_status_done(status):\n",
    "    if status[\"AutoMLJobStatus\"] == \"Completed\":\n",
    "        return True\n",
    "    elif status[\"AutoMLJobStatus\"] in (\"Failed\", \"Stopped\"):\n",
    "        raise ValueError(f\"Job ended in non-successful state '{status['AutoMLJobStatus']}'\\n{status}\")\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "util.progress.polling_spinner(\n",
    "    autoestimator.describe_auto_ml_job,\n",
    "    is_automl_status_done,\n",
    "    fn_stringify_result=lambda status: f\"{status['AutoMLJobStatus']} - {status['AutoMLJobSecondaryStatus']}\",\n",
    "    spinner_secs=0.4,\n",
    "    poll_secs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging in Our Experiment\n",
    "\n",
    "Autopilot always creates a **Experiment** with associated Trials and Trial Components describing the detail of the flow it undertook.\n",
    "\n",
    "For the purposes of **our Experiment** (as created in Notebook 1) which is to compare Autopilot with other methods, the Autopilot run is just one Trial and we only care about the best/selected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe_auto_ml_job() doesn't seem to give us anything to reconstruct what the Experiment name is, so\n",
    "# we'll assume it was created with the AutoML job name + standard suffix:\n",
    "automl_experiment = Experiment.load(f\"{auto_ml_job_name}-aws-auto-ml-job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract relevant data from the 'best' Trial/Components of AutoML Experiment, and copy the info to a Trial in our Experiment\n",
    "list(Trial.load(list(automl_experiment.list_trials())[0].trial_name).list_trial_components())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoestimator.deploy(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
