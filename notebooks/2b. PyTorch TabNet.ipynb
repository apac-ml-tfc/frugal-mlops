{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest CoverType 2b): PyTorch TabNet\n",
    "\n",
    "In this notebook, we'll tackle our Forest Cover Type classification problem using [**PyTorch-TabNet**](https://github.com/dreamquark-ai/tabnet): A PyTorch-based implementation of TabNet (Arik, S. O., & Pfister, T. (2019). TabNet: Attentive Interpretable Tabular Learning. arXiv preprint arXiv:1908.07442.) https://arxiv.org/pdf/1908.07442.pdf\n",
    "\n",
    "To grossly over-simplify: TabNet applies deep learning techniques that have recently shown revolutionary success in NLP (attention, transformers, unsupervised pre-training via masking) to tabular problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and config (yawn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import json\n",
    "import os\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore import exceptions as botoexceptions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.pytorch.estimator import PyTorch as PyTorchEstimator\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "import seaborn as sn\n",
    "from sklearn import metrics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "# Local Dependencies:\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r experiment_name\n",
    "%store -r preproc_trial_component_name\n",
    "%store -r project_id\n",
    "%store -r target_model\n",
    "\n",
    "smclient = boto3.client(\"sagemaker\")\n",
    "role = sagemaker.get_execution_role()\n",
    "smsess = sagemaker.session.Session()\n",
    "\n",
    "project = util.project.init(project_id, role)\n",
    "print(project)\n",
    "\n",
    "sandbox_bucket = boto3.resource(\"s3\").Bucket(project.sandbox.sandbox_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "For the purposes of our **Experiment**, the (best outcome of the) PyTorch approach is one trial to be compared against other qualitatively different approaches.\n",
    "\n",
    "If we were planning to iterate a lot on TabNet parameters, we may also decide to define **another Experiment** to capture our tests leading up to the higher-level business outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_trial = Trial.create(\n",
    "    trial_name=util.append_timestamp(\"tabnet-pytorch\"), \n",
    "    experiment_name=experiment_name,\n",
    "    sagemaker_boto_client=smclient,\n",
    ")\n",
    "tabnet_trial.add_trial_component(preproc_trial_component_name)\n",
    "\n",
    "preproc_trial_component = TrialComponent.load(preproc_trial_component_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR load existing trial instead, e.g:\n",
    "#tabnet_trial = Trial.load(\"tabnet-pytorch-2020-07-28-04-56-23\")\n",
    "#preproc_trial_component = TrialComponent.load(preproc_trial_component_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of the Estimator should be quite familiar to experienced framework container users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"model-type\": \"classification\",\n",
    "    \"target\": \"Cover_Type\",\n",
    "    \"seed\": 1337,\n",
    "    \"n-d\": 64,\n",
    "    \"n-a\": 64,\n",
    "    \"n-steps\": 5,\n",
    "    \"lr\": 0.02,\n",
    "    \"gamma\": 1.5,\n",
    "    \"n-independent\": 2,\n",
    "    \"n-shared\": 2,\n",
    "    # TODO: Could integrate some additional hyperparams...\n",
    "    #\"cat-idxs\": \",\".join(map(lambda i: str(i), cat_idxs)),\n",
    "    # cat-dims???\n",
    "    #\"cat-emb-dim\": \",\".join(map(lambda i: str(i), cat_emb_dim)),\n",
    "    \"lambda-sparse\": 1e-4,\n",
    "    \"momentum\": 0.3,\n",
    "    \"clip-value\": 2.,\n",
    "    \"max-epochs\": 200,  # Try 1000 for accuracy\n",
    "    \"patience\": 100,\n",
    "    \"batch-size\": 16384,\n",
    "    \"virtual-batch-size\": 256,\n",
    "    \"num-workers\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "estimator = PyTorchEstimator(\n",
    "    role=role,\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"src\",\n",
    "    # Anecdotally, have seen accuracy degrade on 1.5.1 (88%) vs 1.4 (96%)... And v1.6 was in the middle (93%)\n",
    "    framework_version=\"1.4\",\n",
    "    py_version=\"py3\",\n",
    "\n",
    "    base_job_name=\"forestcover-tabnet\",\n",
    "    output_path=f\"s3://{sandbox_bucket.name}/trainjobs\",\n",
    "    checkpoint_s3_uri=f\"s3://{sandbox_bucket.name}/trainjobs\",\n",
    "\n",
    "    debugger_hook_config=False,\n",
    "\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=[\n",
    "        # One console log per output e.g.:\n",
    "        # | EPOCH | train | valid | total time (s)\n",
    "        # | 1 | 0.58782 | 0.06811 | 25.5\n",
    "        # Since these rows are a bit brusque, we'll write quite precise/picky regexs to stay safe:\n",
    "        { \"Name\": \"train:accuracy\", \"Regex\": r\"\\| +\\d+ +\\| +(.*?) +\\| +[^\\s]+ +\\| +[^\\s]+\", },\n",
    "        { \"Name\": \"validation:accuracy\", \"Regex\": r\"\\| +\\d+ +\\| +[^\\s]+ +\\| +(.*?) +\\| +[^\\s]+\", },\n",
    "    ],\n",
    "    enable_sagemaker_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...But one important difference is the `experiment_config` parameter for `.fit()` - which will automatically create special training job-linked Trial Components for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    inputs={\n",
    "        \"train\": preproc_trial_component.output_artifacts[\"train-csv\"].value,\n",
    "        \"validation\": preproc_trial_component.output_artifacts[\"validation-csv\"].value,\n",
    "    },\n",
    "    experiment_config={\n",
    "        # This will create a TrainingJob-linked TrialComponent and automatically attach hyperparameters etc\n",
    "        \"TrialName\": tabnet_trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": \"Training\",\n",
    "    },\n",
    "    #wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OR re-attach to previous training job instead, e.g:\n",
    "#estimator = estimator.attach(\"forestcover-tabnet-2020-07-28-05-11-17-070\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model (Batch Transform)\n",
    "\n",
    "We've trained something, and we have validation metrics which should give us an idea of its performance: But at some point we probably want to test out inference to validate that everything's working OK (and maybe see what performance we can expect on our test set).\n",
    "\n",
    "In particular, this model needs a [src/inference.py](src/inference.py) script to:\n",
    "- Meet the I/O requirements of the `DefaultModelMonitor` we'll use later, and\n",
    "- Tell the PyTorch container how to load the model, since pytorch-tabnet currently only offers saving via Pickle (not standard Pytorch model save)\n",
    "\n",
    "Rather than using the **\"one-click deploy\"** APIs `Estimator.transformer()` and `Estimator.deploy()`, we'll explicitly create a `Model` in SageMaker to give us a little more control over our experiment's lifecycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_desc = estimator.latest_training_job.describe()\n",
    "model_path = training_job_desc[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "model_name = training_job_desc[\"TrainingJobName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.delete_model()\n",
    "except (AttributeError, NameError, ValueError):\n",
    "    # AttributeError: Model() wasn't initialized with a sagemaker_session and hasn't created one yet\n",
    "    # NameError: model hasn't been defined yet\n",
    "    # ValueError: Current model isn't saved to SageMaker\n",
    "    pass\n",
    "except botoexceptions.ClientError as e:\n",
    "    if (\n",
    "        e.response[\"Error\"][\"Code\"] == \"ValidationException\"\n",
    "        and e.response[\"Error\"][\"Message\"].startswith(\"Could not find\")\n",
    "    ):\n",
    "        # SDK tried to delete but model wasn't found\n",
    "        pass\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Note a SageMaker SDK \"Model\" isn't strictly backed by a SageMaker API \"Model\": The Model only gets created\n",
    "# when .transformer() or .deploy() are called... So we can't apply tags at the point of SDK \"Model\" creation\n",
    "# or load an SDK \"Model\" from an API Model name/ARN.\n",
    "model = PyTorchModel(\n",
    "    name=model_name,\n",
    "    model_data=model_path,\n",
    "    role=role,\n",
    "    source_dir=\"src/\",\n",
    "    entry_point=\"src/inference.py\",\n",
    "    framework_version=\"1.4\",\n",
    "    py_version=\"py3\",\n",
    "    sagemaker_session=smsess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",  # g4dn family not yet supported for batch transform\n",
    "    strategy=\"MultiRecord\",\n",
    "    max_payload=1,  # 1MB\n",
    "    accept=\"text/csv\",  # Need to specify input and output types when using filters for .transform()\n",
    "    assemble_with=\"Line\",\n",
    "    output_path=f\"s3://{sandbox_bucket.name}/test/{model.name}\",\n",
    ")\n",
    "\n",
    "# The API model is created now, so let's tag it straight away with Boto3:\n",
    "model_desc = smclient.describe_model(ModelName=transformer.model_name)\n",
    "smclient.add_tags(\n",
    "    ResourceArn=model_desc[\"ModelArn\"],\n",
    "    Tags=[\n",
    "        { \"Key\": \"ExperimentName\", \"Value\": experiment_name },\n",
    "        { \"Key\": \"TrialName\", \"Value\": tabnet_trial.trial_name },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use SageMaker Batch Transform's [data processing functionality](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html) to control how data gets batched in to our algorithm, and [associate output predictions with the input data](https://aws.amazon.com/blogs/machine-learning/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform/) - which will make measuring accuracy much easier later.\n",
    "\n",
    "As with the training job, providing an `experiment_config` will enable the transform job to create us a **Trial Component** recording the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer.transform(\n",
    "    preproc_trial_component.output_artifacts[\"test-csv\"].value,\n",
    "    split_type=\"Line\",\n",
    "    content_type=\"text/csv\",  # Need to specify input and output types when using filters\n",
    "    # TODO: Check why -2 is required vs -1 per JSONPath spec for trimming last column\n",
    "    input_filter=\"$[:-2]\",  # Exclude target column from input to the model\n",
    "    join_source=\"Input\",  # Store both input and output in the result (saves us re-joining in notebook)\n",
    "    # No output_filter so our output will be all source columns (incl target) + all prediction columns\n",
    "    experiment_config={\n",
    "        \"ExperimentName\": experiment_name,\n",
    "        \"TrialName\": tabnet_trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": \"Test\",\n",
    "    },\n",
    "    wait=True,\n",
    "    logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the transform complete, we simply need to download and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_root_filename = preproc_trial_component.output_artifacts[\"test-csv\"].value.rpartition(\"/\")[2]\n",
    "\n",
    "!mkdir -p data/test/$model_name\n",
    "sandbox_bucket.download_file(\n",
    "    f\"test/{model.name}/{test_root_filename}.out\",  # Batch Transform appends \".out\"\n",
    "    f\"data/test/{model.name}/{test_root_filename}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/columns.json\", \"r\") as f:\n",
    "    train_columns = json.load(f)\n",
    "\n",
    "# TODO: Save from data prep, as we did with columns\n",
    "# Note our first cover_type is a dummy because the dataset's encoding starts at 1.\n",
    "cover_types = (\"N/A\", \"Spruce/Fir\", \"Lodgepole Pine\", \"Ponderosa Pine\", \"Cottonwood/Willow\", \"Aspen\", \"Douglas-fir\", \"Krummholz\")\n",
    "\n",
    "result_cols = train_columns[:-1] + [\"Actual_Cover_Type\"] + [\"Pred \" + typ for typ in cover_types[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_results = pd.read_csv(\n",
    "    f\"data/test/{model.name}/{test_root_filename}\",\n",
    "    names=result_cols\n",
    ")\n",
    "\n",
    "print(f\"Shape: {df_test_results.shape}\")\n",
    "df_test_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results just have class probabilities: Recover the predicted class column (argmax):\n",
    "df_pred_probs = df_test_results[[\"Pred \" + typ for typ in cover_types[1:]]]\n",
    "# idxmax axis=1 returns index values (i.e. column names) - so rename columns to numbers:\n",
    "predicted_classes = df_pred_probs.rename(\n",
    "    columns={x:y for x,y in zip(df_pred_probs.columns,range(1,1+len(df_pred_probs.columns)))}\n",
    ").idxmax(axis=1)\n",
    "\n",
    "# Now we can add the post-processed results into the main dataframe:\n",
    "df_test_results[\"Pred_Cover_Type\"] = predicted_classes\n",
    "df_test_results[\"Pred_Correct\"] = df_test_results[\"Pred_Cover_Type\"] == df_test_results[\"Actual_Cover_Type\"]\n",
    "df_test_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix and accuracy summary:\n",
    "\n",
    "confusion = metrics.confusion_matrix(df_test_results[\"Actual_Cover_Type\"], df_test_results[\"Pred_Cover_Type\"])\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(\n",
    "    pd.DataFrame(\n",
    "        confusion,\n",
    "        index = cover_types[1:],\n",
    "        columns = cover_types[1:],\n",
    "    ),\n",
    "    annot=True\n",
    ")\n",
    "\n",
    "n_correct = sum(df_test_results[\"Pred_Correct\"])\n",
    "n_tested = len(df_test_results)\n",
    "print(f\"{n_correct} of {n_tested} samples correct: Accuracy={n_correct/n_tested:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Interactive ROC curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Associate accuracy metric with existing Test TrialComponent?\n",
    "\n",
    "# It's not trivial because neither training nor processing jobs (currently) advertise what TrialComponentName\n",
    "# they created during run: We only have access to the DisplayName we requested... So could work around by\n",
    "# appending timestamps to our displayname (to make lookups unique when job retried multiple times within\n",
    "# trial), but that would make the structure within a trial less straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register/submit the model for deployment\n",
    "\n",
    "Now we have (in our data science sandbox) a candidate model which is performing well. We'd like to promote it up to the project space and deploy it to an environment.\n",
    "\n",
    "How we set up our \"model registry\" depends on the governance requirements of the project and organization. Note that while the `ModelPackage` and `Algorithm` constructs built in to SageMaker are one interesting option, they don't currently support resource tagging. :-("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_result = project.submit_model(\n",
    "    {\n",
    "        \"EndpointName\": target_model,\n",
    "        \"TrainingJob\": training_job_desc,\n",
    "        \"Model\": model_desc,\n",
    "    },\n",
    "    wait=True\n",
    ")\n",
    "print(submission_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: A quick demo of real-time deployment\n",
    "\n",
    "Sure our main workflow is to test the model with Batch Transform - but what about cases where the batch transform feed format might be slightly different from production?\n",
    "\n",
    "If we wanted to quickly experiment with deploying our model to a test endpoint here, it could look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    predictor.delete_endpoint()\n",
    "    time.sleep(5)  # (Otherwise can trigger ResourceLimitExceeded if creating again immediately)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "predictor = model.deploy(\n",
    "    endpoint_name=model.name,  # Use model.name to avoid us accidentally deploying the model twice\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    #wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/columns.json\", \"r\") as f:\n",
    "    train_columns = json.load(f)\n",
    "df_test = pd.read_csv(\n",
    "    \"data/test-noheader.csv\",\n",
    "    names=train_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, PyTorch predictors use serializer sagemaker.predictor.npy_serializer and content_type\n",
    "# application/x-npy ...So it's as easy as dropping the target column and converting pandas->numpy.\n",
    "# (Note you can do many more than 10 samples at once - see the batch transform logs)\n",
    "result = predictor.predict(df_test.drop(\"Cover_Type\", axis=1).iloc[0:10].to_numpy())\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
