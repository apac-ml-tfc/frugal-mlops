{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest CoverType 2b): PyTorch TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import json\n",
    "import os\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore import exceptions as botoexceptions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.pytorch.estimator import PyTorch as PyTorchEstimator\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "import seaborn as sn\n",
    "from sklearn import metrics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "# Local Dependencies:\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r bucket_name\n",
    "%store -r experiment_name\n",
    "%store -r preproc_trial_component_name\n",
    "\n",
    "bucket = boto3.resource(\"s3\").Bucket(bucket_name)\n",
    "role = sagemaker.get_execution_role()\n",
    "smclient = boto3.client(\"sagemaker\")\n",
    "smsess = sagemaker.session.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_trial = Trial.create(\n",
    "    trial_name=util.append_timestamp(\"tabnet-pytorch\"), \n",
    "    experiment_name=experiment_name,\n",
    "    sagemaker_boto_client=smclient,\n",
    ")\n",
    "tabnet_trial.add_trial_component(preproc_trial_component_name)\n",
    "\n",
    "preproc_trial_component = TrialComponent.load(preproc_trial_component_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"model-type\": \"classification\",\n",
    "    \"target\": \"Cover_Type\",\n",
    "    \"seed\": 1337,\n",
    "    \"n-d\": 64,\n",
    "    \"n-a\": 64,\n",
    "    \"n-steps\": 5,\n",
    "    \"lr\": 0.02,\n",
    "    \"gamma\": 1.5,\n",
    "    \"n-independent\": 2,\n",
    "    \"n-shared\": 2,\n",
    "    #\"cat-idxs\": \",\".join(map(lambda i: str(i), cat_idxs)),\n",
    "    # cat-dims???\n",
    "    #\"cat-emb-dim\": \",\".join(map(lambda i: str(i), cat_emb_dim)),\n",
    "    \"lambda-sparse\": 1e-4,\n",
    "    \"momentum\": 0.3,\n",
    "    \"clip-value\": 2.,\n",
    "    \"max-epochs\": 500,  # Try 1000 for accuracy\n",
    "    \"patience\": 100,\n",
    "    \"batch-size\": 16384,\n",
    "    \"virtual-batch-size\": 256,\n",
    "    \"num-workers\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "estimator = PyTorchEstimator(\n",
    "    role=role,\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"src\",\n",
    "    framework_version=\"1.4\",\n",
    "\n",
    "    base_job_name=\"forestcover-tabnet\",\n",
    "    output_path=f\"s3://{bucket_name}/trainjobs\",\n",
    "    checkpoint_s3_uri=f\"s3://{bucket_name}/trainjobs\",\n",
    "\n",
    "    debugger_hook_config=False,\n",
    "\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.p3.2xlarge\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=[\n",
    "        # One console log per output e.g.:\n",
    "        # | EPOCH | train | valid | total time (s)\n",
    "        # | 1 | 0.58782 | 0.06811 | 25.5\n",
    "        # Since these rows are a bit brusque, we'll write quite precise/picky regexs to stay safe:\n",
    "        { \"Name\": \"train:accuracy\", \"Regex\": r\"\\| +\\d+ +\\| +(.*?) +\\| +[^\\s]+ +\\| +[^\\s]+\", },\n",
    "        { \"Name\": \"validation:accuracy\", \"Regex\": r\"\\| +\\d+ +\\| +[^\\s]+ +\\| +(.*?) +\\| +[^\\s]+\", },\n",
    "    ],\n",
    "    enable_sagemaker_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    inputs={\n",
    "        \"train\": preproc_trial_component.output_artifacts[\"train-csv\"].value,\n",
    "        \"validation\": preproc_trial_component.output_artifacts[\"validation-csv\"].value,\n",
    "    },\n",
    "    experiment_config={\n",
    "        # This will create a TrainingJob-linked TrialComponent and automatically attach hyperparameters etc\n",
    "        \"TrialName\": tabnet_trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": \"Training\",\n",
    "    },\n",
    "    #wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_desc = estimator.latest_training_job.describe()\n",
    "model_path = training_job_desc[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "model_name = training_job_desc[\"TrainingJobName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.delete_model()\n",
    "except (AttributeError, NameError, ValueError):\n",
    "    # AttributeError: Model() wasn't initialized with a sagemaker_session and hasn't created one yet\n",
    "    # NameError: model hasn't been defined yet\n",
    "    # ValueError: Current model isn't saved to SageMaker\n",
    "    pass\n",
    "except botoexceptions.ClientError as e:\n",
    "    if (\n",
    "        e.response[\"Error\"][\"Code\"] == \"ValidationException\"\n",
    "        and e.response[\"Error\"][\"Message\"].startswith(\"Could not find\")\n",
    "    ):\n",
    "        # SDK tried to delete but model wasn't found\n",
    "        pass\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "model = PyTorchModel(\n",
    "    name=model_name,\n",
    "    model_data=model_path,\n",
    "    role=role,\n",
    "    source_dir=\"src/\",\n",
    "    entry_point=\"src/inference.py\",\n",
    "    framework_version=\"1.4\",\n",
    "    sagemaker_session=smsess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",  # g4dn family not yet supported for batch transform\n",
    "    strategy=\"MultiRecord\",\n",
    "    max_payload=1,  # 1MB\n",
    "    accept=\"text/csv\",  # Need to specify input and output types when using filters\n",
    "    assemble_with=\"Line\",\n",
    "    output_path=f\"s3://{bucket_name}/test/{model.name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer.transform(\n",
    "    preproc_trial_component.output_artifacts[\"test-csv\"].value,\n",
    "    split_type=\"Line\",\n",
    "    content_type=\"text/csv\",  # Need to specify input and output types when using filters\n",
    "    # TODO: Check why -2 is required vs -1 per JSONPath spec for trimming last column\n",
    "    input_filter=\"$[:-2]\",  # Exclude target column from input to the model\n",
    "    join_source=\"Input\",  # Store both input and output in the result (saves us re-joining in notebook)\n",
    "    # No output_filter so our output will be all source columns (incl target) + all prediction columns\n",
    "    experiment_config={\n",
    "        \"ExperimentName\": experiment_name,\n",
    "        \"TrialName\": tabnet_trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": \"Test\",\n",
    "    },\n",
    "    wait=True,\n",
    "    logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_root_filename = preproc_trial_component.output_artifacts[\"test-csv\"].value.rpartition(\"/\")[2]\n",
    "\n",
    "!mkdir -p data/test/$model_name\n",
    "bucket.download_file(\n",
    "    f\"test/{model.name}/{test_root_filename}.out\",  # Batch Transform appends \".out\"\n",
    "    f\"data/test/{model.name}/{test_root_filename}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/columns.json\", \"r\") as f:\n",
    "    train_columns = json.load(f)\n",
    "\n",
    "# TODO: Save from data prep\n",
    "# Note our first cover_type is a dummy because the dataset's encoding starts at 1.\n",
    "cover_types = (\"N/A\", \"Spruce/Fir\", \"Lodgepole Pine\", \"Ponderosa Pine\", \"Cottonwood/Willow\", \"Aspen\", \"Douglas-fir\", \"Krummholz\")\n",
    "\n",
    "result_cols = train_columns[:-1] + [\"Actual_Cover_Type\"] + [\"Pred \" + typ for typ in cover_types[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_results = pd.read_csv(\n",
    "    f\"data/test/{model.name}/{test_root_filename}\",\n",
    "    names=result_cols\n",
    ")\n",
    "\n",
    "print(f\"Shape: {df_test_results.shape}\")\n",
    "df_test_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results just have class probabilities: Recover the predicted class column (argmax):\n",
    "df_pred_probs = df_test_results[[\"Pred \" + typ for typ in cover_types[1:]]]\n",
    "# idxmax axis=1 returns index values (i.e. column names) - so rename columns to numbers:\n",
    "predicted_classes = df_pred_probs.rename(\n",
    "    columns={x:y for x,y in zip(df_pred_probs.columns,range(1,1+len(df_pred_probs.columns)))}\n",
    ").idxmax(axis=1)\n",
    "\n",
    "# Now we can add the post-processed results into the main dataframe:\n",
    "df_test_results[\"Pred_Cover_Type\"] = predicted_classes\n",
    "df_test_results[\"Pred_Correct\"] = df_test_results[\"Pred_Cover_Type\"] == df_test_results[\"Actual_Cover_Type\"]\n",
    "df_test_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix and accuracy summary:\n",
    "\n",
    "confusion = metrics.confusion_matrix(df_test_results[\"Actual_Cover_Type\"], df_test_results[\"Pred_Cover_Type\"])\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(\n",
    "    pd.DataFrame(\n",
    "        confusion,\n",
    "        index = cover_types[1:],\n",
    "        columns = cover_types[1:],\n",
    "    ),\n",
    "    annot=True\n",
    ")\n",
    "\n",
    "n_correct = sum(df_test_results[\"Pred_Correct\"])\n",
    "n_tested = len(df_test_results)\n",
    "print(f\"{n_correct} of {n_tested} samples correct: Accuracy={n_correct/n_tested:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Interactive ROC curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Associate accuracy metric with existing Test TrialComponent?\n",
    "\n",
    "# It's not trivial because neither training nor processing jobs seem to advertise what TrialComponentName\n",
    "# they created during run: We only have access to the DisplayName we requested... So could work around by\n",
    "# appending timestamps to our displayname (to make lookups unique when job retried multiple times within\n",
    "# trial), but that would make the structure within a trial less straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Demo\n",
    "\n",
    "To be moved to deployment section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    predictor.delete_endpoint()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "predictor = model.deploy(\n",
    "    endpoint_name=model.name,  # Use model.name to avoid us accidentally deploying the model twice\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    #wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\n",
    "    \"data/test-noheader.csv\",\n",
    "    names=train_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, PyTorch predictors use serializer sagemaker.predictor.npy_serializer and content_type\n",
    "# application/x-npy ...So it's as easy as dropping the target column and converting pandas->numpy.\n",
    "# (Note you can do many more than 10 samples at once - see the batch transform logs)\n",
    "result = predictor.predict(df_test.drop(\"Cover_Type\", axis=1).iloc[0:10].to_numpy())\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
