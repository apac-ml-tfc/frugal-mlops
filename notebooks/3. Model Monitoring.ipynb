{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest CoverType 3): Model Monitoring and Drift Detection\n",
    "\n",
    "We've already covered setting up a pipeline to canary deploy new model variants to an endpoint with data capture enabled...\n",
    "\n",
    "Our next mission is to regularly analyze that captured data to detect drift and trigger remedial activities (e.g. warnings, re-training, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and config..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore import exceptions as botoexceptions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.pytorch.estimator import PyTorch as PyTorchEstimator\n",
    "from sagemaker.pytorch.model import PyTorchModel, PyTorchPredictor\n",
    "import seaborn as sn\n",
    "from sklearn import metrics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "from tqdm import tqdm  # (Progress bars)\n",
    "\n",
    "# Local Dependencies:\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r experiment_name\n",
    "%store -r preproc_trial_component_name\n",
    "%store -r project_id\n",
    "%store -r target_model\n",
    "\n",
    "smclient = boto3.client(\"sagemaker\")\n",
    "role = sagemaker.get_execution_role()\n",
    "smsess = sagemaker.session.Session()\n",
    "\n",
    "project = util.project.init(project_id, role)\n",
    "print(project)\n",
    "\n",
    "sandbox_bucket = boto3.resource(\"s3\").Bucket(project.sandbox.sandbox_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_trial_component = TrialComponent.load(preproc_trial_component_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating some traffic\n",
    "\n",
    "Our TabNet endpoint is quite flexible in the data formats it supports: For example we demonstrated mini-batched inference on `application/x-npy` input at the end of the TabNet notebook.\n",
    "\n",
    "...But we'd like to use the [DefaultModelMonitor](https://sagemaker.readthedocs.io/en/stable/api/inference/model_monitor.html#sagemaker.model_monitor.model_monitoring.DefaultModelMonitor), rather than taking on the challenge of custom a custom monitoring processor at this stage.\n",
    "\n",
    "So for our production endpoint, we'll stick to **single-record requests in text/csv format**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = PyTorchPredictor(\n",
    "    target_model,\n",
    "    # By default, PyTorch predictors use application/x-npy on the wire... Which is nice except default model\n",
    "    # monitor can't understand the captures! So we'll use CSV instead:\n",
    "    serializer=sagemaker.serializers.CSVSerializer(),\n",
    "    deserializer=sagemaker.deserializers.CSVDeserializer(),\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up our test dataframe from local file:\n",
    "with open(\"data/columns.json\", \"r\") as f:\n",
    "    train_columns = json.load(f)\n",
    "\n",
    "df_test = pd.read_csv(\n",
    "    \"data/test-noheader.csv\",\n",
    "    names=train_columns\n",
    ")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below loops through the entire test dataframe exactly once, one record at a time...\n",
    "\n",
    "That can take a long time and we're just trying to generate some sample data, so feel free to interrupt it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "inf_batch_size = 1  # Important to keep = 1 if we want DefaultModelMonitor to work!\n",
    "n_inf_batches = math.ceil(len(df_test) / inf_batch_size)\n",
    "\n",
    "print(f\"Sending test data for inference in {n_inf_batches} batches of {inf_batch_size} records...\")\n",
    "iterator = tqdm(\n",
    "    df_test.drop(\"Cover_Type\", axis=1).groupby(np.arange(len(df_test))//inf_batch_size),\n",
    "    total=n_inf_batches\n",
    ")\n",
    "\n",
    "def predict_batch(it):\n",
    "    ixbatch, group = it\n",
    "    result = np.array(predictor.predict(group.to_numpy()), dtype=\"float\")\n",
    "    time.sleep(0.5)\n",
    "    return result\n",
    "\n",
    "# This utility fn will show us a tqdm progress bar *without* it getting messed up by notebook interrupts\n",
    "last_result = util.progress.notebook_safe_tqdm_loop(\n",
    "    iterator,\n",
    "    predict_batch\n",
    ")\n",
    "\n",
    "print(\"Done!\")\n",
    "print(\"Last batch result:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a DefaultModelMonitor\n",
    "\n",
    "Just like an `Estimator` creating a ModelMonitor doesn't actually *do* anything - we're just defining configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_default_monitor = sagemaker.model_monitor.DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    volume_size_in_gb=5,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Or attach to an endpoint)\n",
    "#my_default_monitor = sagemaker.model_monitor.DefaultModelMonitor.attach(\n",
    "#    target_model\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselining from the training data set\n",
    "\n",
    "First we want to summarize a **baseline** data distribution, to track deviations observed in production.\n",
    "\n",
    "A logical choice in this scenario is to use the use the dataset the model was actually trained on.\n",
    "\n",
    "The `DefaultModelMonitor.suggest_baseline()` function:\n",
    "- Sets up a **SageMaker Processing Job**, which\n",
    "- Analyses **the data set in S3**, to\n",
    "- *Suggest* **summary statistics and constraints** for each field in the data\n",
    "\n",
    "Like any other Processing Job these outputs will appear as files in our S3 output location, but DefaultModelMonitor will give us some additional convenience methods for analysing their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_baseline_uri = \"s3://{}/baselines/{}/{} {}/train\".format(\n",
    "    project.monitoring_bucket,\n",
    "    target_model,\n",
    "    preproc_trial_component.display_name,  # (Not unique)\n",
    "    preproc_trial_component.trial_component_name,  # (Not human-interpretable)\n",
    ")\n",
    "\n",
    "train_baseline_job = my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=preproc_trial_component.output_artifacts[\"train-csv\"].value,\n",
    "    dataset_format=sagemaker.model_monitor.DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=train_baseline_uri,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that these statistics and constraints can be downloaded, inspected **and edited**: If you'd like to impose different alerting constraints from human domain understanding, that's fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/monitoring/baselines\n",
    "baselines_uri = f\"s3://{project.monitoring_bucket}/baselines\"\n",
    "!aws s3 sync $baselines_uri data/monitoring/baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_df = pd.io.json.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "constraints_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselining from live data\n",
    "\n",
    "An **alternative** strategy that might be more relevant to some online learning use-cases where the \"training set\" is less well-defined, is to calculate statistics and proposed constraints from live captured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "live_baseline_uri = \"s3://{}/baselines/{}/live/{}\".format(\n",
    "    project.monitoring_bucket,\n",
    "    target_model,\n",
    "    util.append_timestamp(\"baseline\"),\n",
    ")\n",
    "\n",
    "live_baseline_job = my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=\"s3://{}/capture/{}\".format(\n",
    "        project.monitoring_bucket,\n",
    "        target_model,\n",
    "        # TODO: Will you filter down to a subset of this path?\n",
    "    ),\n",
    "    # We don't need any fancy pre/post processing scripts for this use case:\n",
    "    #record_preprocessor_script=\"src-monitoring/preprocessor.py\",\n",
    "    #post_analytics_processor_script=None,\n",
    "    dataset_format=sagemaker.model_monitor.DatasetFormat.sagemaker_capture_json(),\n",
    "    output_s3_uri=live_baseline_uri,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension Exercise:\n",
    "\n",
    "Can you modify one or more of the suggested constraints, and create your monitoring schedule (below) from that modified copy to trigger additional violations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a monitoring schedule\n",
    "\n",
    "With a baseline established and endpoint data capture configured, we'd like SageMaker to regularly check the data coming through our model and warn us about violations.\n",
    "\n",
    "Since we're using constraint/statistic and data capture formats that the DefaultModelMonitor understands, this is a simple SDK call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_default_monitor.create_monitoring_schedule(\n",
    "    target_model,\n",
    "    # We'll try to avoid accidentally setting up multiple schedules on one endpoint:\n",
    "    monitor_schedule_name=target_model,\n",
    "    # We don't need any fancy pre/post processing scripts for this use case:\n",
    "    #record_preprocessor_script=\"src-monitoring/preprocessor.py\",\n",
    "    #post_analytics_processor_script=None,\n",
    "    output_s3_uri=\"s3://{}/schedule-results/{}\".format(\n",
    "        project.monitoring_bucket,\n",
    "        target_model,\n",
    "    ),\n",
    "    statistics=baseline_job.baseline_statistics(),\n",
    "    constraints=baseline_job.suggested_constraints(),\n",
    "    # Watch out: Only a subset of Cron (scheduling) expressions are supported!\n",
    "    schedule_cron_expression=sagemaker.model_monitor.CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_default_monitor.describe_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some *deliberately biased* traffic\n",
    "\n",
    "remember that biased testing subset we extracted in the first notebook? Let's put it to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biased = pd.read_csv(\n",
    "    \"data/test-bias-noheader.csv\",\n",
    "    names=train_columns\n",
    ")\n",
    "df_biased.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "inf_batch_size = 1  # Important to keep = 1 if we want DefaultModelMonitor to work!\n",
    "n_inf_batches = math.ceil(len(df_biased) / inf_batch_size)\n",
    "\n",
    "print(f\"Sending biased test data for inference in {n_inf_batches} batches of {inf_batch_size} records...\")\n",
    "iterator = tqdm(\n",
    "    df_biased.drop(\"Cover_Type\", axis=1).groupby(np.arange(len(df_biased))//inf_batch_size),\n",
    "    total=n_inf_batches\n",
    ")\n",
    "\n",
    "def predict_batch(it):\n",
    "    ixbatch, group = it\n",
    "    result = np.array(predictor.predict(group.to_numpy()), dtype=\"float\")\n",
    "    time.sleep(0.5)\n",
    "    return result\n",
    "\n",
    "# This utility fn will show us a tqdm progress bar *without* it getting messed up by notebook interrupts\n",
    "last_result = util.progress.notebook_safe_tqdm_loop(\n",
    "    iterator,\n",
    "    predict_batch\n",
    ")\n",
    "\n",
    "print(\"Done!\")\n",
    "print(\"Last batch result:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the results\n",
    "\n",
    "Hopefully our biased traffic has now been running for long enough to collect some good data and trigger a scheduled monitoring job.\n",
    "\n",
    "We can explore what we've collected through the SageMaker Studio UI, but also through the APIs.\n",
    "\n",
    "Let's explore what we've collected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_capture_uri = f\"s3://{project.monitoring_bucket}\"\n",
    "!aws s3 sync $data_capture_uri data/monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_default_monitor.list_executions()  # Hopefully one or more by now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_monitoring_job = my_default_monitor.list_executions()[-1]\n",
    "latest_monitoring_job.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension: Visualize the results\n",
    "\n",
    "Drawing on the public sample code from the official repository below, can you present the violations here in the notebook? This API-driven interaction will be useful practice for using Model Monitor in classic SageMaker Notebook Instances, outside Studio.\n",
    "\n",
    "https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker_model_monitor/introduction/SageMaker-ModelMonitoring.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_default_monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
